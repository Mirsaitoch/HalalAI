"""Chat endpoints –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤."""

import asyncio
import logging
from typing import Any, Dict, List, Optional

import torch
from fastapi import APIRouter, Depends, HTTPException

from halal_ai.api.dependencies import get_local_llm, get_rag_pipeline
from halal_ai.core import ALLOWED_MESSAGE_ROLES, llm_config, rag_config
from halal_ai.core.exceptions import RemoteLLMException, TimeoutException
from halal_ai.models import ChatRequest, ChatResponse, RemoteTestRequest
from halal_ai.services.llm import (
    LocalLLM,
    call_remote_llm,
    get_remote_skip_reason,
    select_remote_model,
    should_use_remote_llm,
)
from halal_ai.services.monitoring import quality_checker
from halal_ai.services.prompts import (
    ensure_system_prompt,
    inject_halal_guardrail,
    inject_rag_context,
    inject_surah_guardrail,
)
from halal_ai.services.rag import RAGPipeline
from halal_ai.utils import (
    build_rag_filters,
    extract_last_user_query,
    generate_query_variants,
    get_rag_relevance_keywords,
    normalize_food_query,
    serialize_sources,
)

router = APIRouter(tags=["chat"])
logger = logging.getLogger(__name__)


async def execute_with_timeout(coro, stage: str):
    """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ—Ä—É—Ç–∏–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏."""
    try:
        return await asyncio.wait_for(coro, timeout=llm_config.REQUEST_TIMEOUT_SECONDS)
    except asyncio.TimeoutError:
        logger.error("%s –ø—Ä–µ–≤—ã—Å–∏–ª–∞ –ª–∏–º–∏—Ç %s —Å–µ–∫—É–Ω–¥.", stage, llm_config.REQUEST_TIMEOUT_SECONDS)
        raise HTTPException(
            status_code=504,
            detail=f"{stage} timed out after {llm_config.REQUEST_TIMEOUT_SECONDS} seconds",
        )


def check_response_quality(content: str, rag_sources: List[Dict[str, Any]]) -> None:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã.
    
    Args:
        content: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏
        rag_sources: –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ RAG
    """
    if not rag_sources:
        # –ï—Å–ª–∏ –Ω–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–∞
        return
    
    quality_report = quality_checker.check_response(content, rag_sources)
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
    if quality_report["quality"] in ["poor", "critical"]:
        logger.warning(
            "üö® –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞! Quality: %s, Risk score: %d",
            quality_report["quality"],
            quality_report["risk_score"],
        )
        
        if quality_report["issues"]:
            for issue in quality_report["issues"]:
                logger.warning("  ‚ö†Ô∏è  %s", issue)
    
    # –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
    citation_info = quality_report["citation_validation"]
    if not citation_info["all_valid"]:
        logger.warning(
            "‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã: %d –∏–∑ %d",
            len(citation_info["invalid_citations"]),
            citation_info["total_citations"],
        )
        for invalid_cite in citation_info["invalid_citations"]:
            logger.warning(
                "   üìñ –ù–µ–≤–∞–ª–∏–¥–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞: —Å—É—Ä–∞ %d, –∞—è—Ç %d",
                invalid_cite["surah"],
                invalid_cite["ayah"],
            )


def sanitize_max_tokens(value: Optional[int]) -> int:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ max_tokens."""
    if value is None:
        return llm_config.MAX_NEW_TOKENS
    return max(llm_config.MIN_NEW_TOKENS, min(value, llm_config.MAX_NEW_TOKENS))


def sanitize_top_k(value: Optional[int]) -> int:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ RAG-–∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤."""
    if value is None:
        return rag_config.DEFAULT_TOP_K
    return max(1, min(value, 10))


def prepare_messages(request: ChatRequest) -> List[Dict[str, str]]:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∑–∞–ø—Ä–æ—Å–∞."""
    normalized: List[Dict[str, str]] = []

    if request.messages:
        for msg in request.messages:
            role = (msg.role or "").strip().lower()
            if role not in ALLOWED_MESSAGE_ROLES:
                raise HTTPException(status_code=400, detail=f"Unsupported role '{msg.role}'")
            content = (msg.content or "").strip()
            if not content:
                continue
            normalized.append({"role": role, "content": content})
    elif request.prompt:
        normalized.append({"role": "user", "content": request.prompt.strip()})
    else:
        raise HTTPException(status_code=400, detail="Either 'prompt' or 'messages' must be provided")

    if not normalized:
        raise HTTPException(status_code=400, detail="Message list is empty after normalization")

    with_system = ensure_system_prompt(normalized)
    guarded = inject_halal_guardrail(with_system)
    return guarded


async def retrieve_rag_context(
    query_text: str,
    rag_top_k: int,
    pipeline: Optional[RAGPipeline],
) -> List[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω."""
    if not rag_config.ENABLED:
        logger.info("RAG –æ—Ç–∫–ª—é—á–µ–Ω —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.")
        return []
    
    if pipeline is None:
        logger.info("RAG –∑–∞–ø—Ä–æ—à–µ–Ω, –Ω–æ –ø–∞–π–ø–ª–∞–π–Ω –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return []
    
    if pipeline.document_count == 0:
        logger.info("RAG –≤–∫–ª—é—á–µ–Ω, –Ω–æ –∏–Ω–¥–µ–∫—Å –ø—É—Å—Ç. –¢—Ä–µ–±—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è.")
        return []
    
    if not query_text:
        logger.info("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è RAG.")
        return []

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    query_variants = generate_query_variants(query_text)
    logger.info("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ %d –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è RAG –ø–æ–∏—Å–∫–∞", len(query_variants))
    logger.info("–í–∞—Ä–∏–∞–Ω—Ç—ã: %s", query_variants[:3])  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
    
    rag_filters = build_rag_filters(query_text)
    
    # –ò—â–µ–º –ø–æ –≤—Å–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    all_sources = []
    seen_ids = set()
    
    for idx, variant in enumerate(query_variants):
        logger.info("–ò—â–µ–º –ø–æ –≤–∞—Ä–∏–∞–Ω—Ç—É #%d: '%s'", idx + 1, variant[:60])
        
        if rag_filters and idx == 0:  # –§–∏–ª—å—Ç—Ä—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ (–æ—Å–Ω–æ–≤–Ω–æ–≥–æ) –≤–∞—Ä–∏–∞–Ω—Ç–∞
            logger.info("–ü–æ–ø—ã—Ç–∫–∞ RAG —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏: %s", rag_filters)
            sources = await asyncio.to_thread(
                pipeline.retrieve,
                variant,
                top_k=rag_top_k,
                filters=rag_filters or None,
                search_top_k=rag_config.SEARCH_TOP_K,
            )
        else:
            sources = await asyncio.to_thread(
                pipeline.retrieve,
                variant,
                top_k=rag_top_k,
                search_top_k=rag_config.SEARCH_TOP_K,
            )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        for source in sources:
            source_id = source.get("id")
            if source_id and source_id not in seen_ids:
                seen_ids.add(source_id)
                all_sources.append(source)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –º–æ–∂–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
        if len(all_sources) >= rag_top_k * 2:
            logger.info("–°–æ–±—Ä–∞–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (%d), –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–∏—Å–∫", len(all_sources))
            break
    
    # –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É —á–∞–Ω–∫–æ–≤, –≥–¥–µ –µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞
    relevance_keywords = get_rag_relevance_keywords(query_text)
    if relevance_keywords:
        with_keyword = []
        without_keyword = []
        for src in all_sources:
            text = (src.get("text") or "").lower()
            if any(kw in text for kw in relevance_keywords):
                with_keyword.append(src)
            else:
                without_keyword.append(src)
        with_keyword.sort(key=lambda x: x.get("score", 0), reverse=True)
        without_keyword.sort(key=lambda x: x.get("score", 0), reverse=True)
        all_sources = with_keyword + without_keyword
        if with_keyword:
            logger.info(
                "RAG: %d –∏–∑ %d –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)",
                len(with_keyword),
                len(all_sources),
            )
    else:
        all_sources.sort(key=lambda x: x.get("score", 0), reverse=True)

    rag_sources = all_sources[:rag_top_k]
    logger.info("RAG –≤–µ—Ä–Ω—É–ª %d —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ %d –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö", len(rag_sources), len(all_sources))
    return rag_sources


async def handle_chat_request(
    request: ChatRequest,
    llm: LocalLLM,
    pipeline: Optional[RAGPipeline],
) -> ChatResponse:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞, RAG, remote/local)."""
    logger.info(
        "–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: prompt_provided=%s, messages=%s",
        bool(request.prompt),
        len(request.messages) if request.messages else 0,
    )

    try:
        messages = prepare_messages(request)
        messages = llm.limit_history_length(messages)
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏—Å—Ç–æ—Ä–∏—è –∏–∑ %s —Å–æ–æ–±—â–µ–Ω–∏–π (–ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)", len(messages))

        rag_sources: List[Dict[str, Any]] = []
        remote_error: Optional[str] = None

        if request.use_rag:
            query_text = extract_last_user_query(messages)
            rag_top_k = sanitize_top_k(request.rag_top_k)
            rag_sources = await retrieve_rag_context(query_text, rag_top_k, pipeline)

            if rag_sources:
                surah_numbers = [
                    ctx.get("metadata", {}).get("surah")
                    for ctx in rag_sources
                    if ctx.get("metadata", {}).get("surah") is not None
                ]
                if surah_numbers:
                    messages = inject_surah_guardrail(messages, surah_numbers)
                messages = inject_rag_context(messages, rag_sources)
                logger.info("–î–æ–±–∞–≤–ª–µ–Ω–æ %s –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ RAG.", len(rag_sources))

        max_new_tokens = sanitize_max_tokens(request.max_tokens)

        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–¥–∞–ª–µ–Ω–Ω—É—é LLM –µ—Å–ª–∏ –µ—Å—Ç—å API –∫–ª—é—á
        if should_use_remote_llm(request.api_key):
            remote_model = select_remote_model(request.remote_model)
            logger.info(
                "–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–¥–∞–ª–µ–Ω–Ω—É—é LLM –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º—É –∫–ª—é—á—É (model=%s, max_tokens=%s).",
                remote_model,
                max_new_tokens,
            )
            try:
                content = await asyncio.to_thread(
                    call_remote_llm,
                    messages,
                    max_new_tokens,
                    request.api_key,
                    remote_model,
                )
                logger.info("–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ [remote:%s]: %s —Å–∏–º–≤–æ–ª–æ–≤", remote_model, len(content))
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
                check_response_quality(content, rag_sources)
                
                return ChatResponse(
                    reply=content,
                    sources=serialize_sources(rag_sources) or None,
                    model=f"remote:{remote_model}",
                    used_remote=True,
                    remote_error="",
                )
            except RemoteLLMException as remote_exc:
                logger.info(
                    "–£–¥–∞–ª–µ–Ω–Ω–∞—è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–ª–∏ –≤–µ—Ä–Ω—É–ª–∞ –æ—à–∏–±–∫—É (%s). –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å.",
                    remote_exc,
                )
                remote_error = str(remote_exc)
        else:
            reason = get_remote_skip_reason(request.api_key)
            if reason:
                logger.info("Remote LLM –ø—Ä–æ–ø—É—â–µ–Ω: %s", reason)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        content = await asyncio.to_thread(llm.generate, messages, max_new_tokens)
        if not content:
            logger.info("–û—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—É—Å—Ç–æ–π.")
        logger.info("–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ [local:%s]: %s —Å–∏–º–≤–æ–ª–æ–≤", llm.model_name, len(content))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
        check_response_quality(content, rag_sources)
        
        return ChatResponse(
            reply=content,
            sources=serialize_sources(rag_sources) or None,
            model=f"local:{llm.model_name}",
            used_remote=False,
            remote_error=remote_error or "",
        )

    except HTTPException:
        raise
    except RuntimeError as runtime_error:
        error_message = str(runtime_error).lower()
        if "out of memory" in error_message:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.error("CUDA out of memory –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", runtime_error)
            raise HTTPException(status_code=500, detail="LLM ran out of memory during generation")
        logger.error("RuntimeError –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", runtime_error)
        raise HTTPException(status_code=500, detail=str(runtime_error))
    except Exception as exc:
        logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", exc)
        raise HTTPException(status_code=500, detail=f"Generation error: {str(exc)}")


@router.post("/chat", response_model=ChatResponse, response_model_exclude_none=False)
async def chat(
    request: ChatRequest,
    llm: LocalLLM = Depends(get_local_llm),
    pipeline: Optional[RAGPipeline] = Depends(get_rag_pipeline),
):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–ª–∏ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π."""
    try:
        return await execute_with_timeout(
            handle_chat_request(request, llm, pipeline),
            "LLM generation",
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", exc)
        raise HTTPException(status_code=500, detail=f"Generation error: {str(exc)}")


@router.post("/remote/test")
async def remote_test(payload: RemoteTestRequest):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —É–¥–∞–ª–µ–Ω–Ω–æ–π LLM —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º api_key (diag endpoint)."""
    reason = get_remote_skip_reason(payload.api_key)
    if reason:
        raise HTTPException(status_code=400, detail=f"Remote LLM skipped: {reason}")
    
    try:
        remote_model = select_remote_model(payload.model)
        messages = [
            {"role": "system", "content": "–¢—ã ‚Äî HalalAI. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º."},
            {"role": "user", "content": payload.prompt.strip() or "–ü–∏–Ω–≥"},
        ]
        content = await asyncio.to_thread(
            call_remote_llm,
            messages,
            sanitize_max_tokens(payload.max_tokens),
            payload.api_key,
            remote_model,
        )
        return {"status": "ok", "reply": content, "model": remote_model}
    except Exception as exc:
        logger.error("Remote test failed: %s", exc)
        raise HTTPException(status_code=502, detail=f"Remote test failed: {exc}")
